{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc1ca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:\n",
      "----------------------------------------\n",
      "Text:  Hey guys, this is Dharinesh. I'm here to talk about myself. So, I'm studying... meaning to study data science. Yeah, this is very nice.\n",
      "\n",
      "Detailed Chunks with Timestamps:\n",
      "----------------------------------------\n",
      "[ 0.0s -  2.5s]:  Hey guys, this is Dharinesh.\n",
      "[ 2.5s -  7.0s]:  I'm here to talk about myself.\n",
      "[ 7.0s -  9.0s]:  So, I'm studying...\n",
      "[ 9.0s - 13.0s]:  meaning to study data science.\n",
      "[13.0s - 15.0s]:  Yeah, this is very nice.\n",
      "\n",
      "üìä Analysis:\n",
      "Languages detected: ['en']\n",
      "‚úì Filler words/patterns detected: ['ah', 'so', 'er', 'yeah']\n",
      "‚úì Speech patterns detected: ['hesitation/pause (...)', 'self-correction', \"'so' as filler\", \"'yeah' as filler\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Hugging Face client\n",
    "client = InferenceClient(\n",
    "    provider=\"fal-ai\",\n",
    "    api_key=os.getenv(\"HF_TOKEN\")\n",
    ")\n",
    "\n",
    "# Path to your audio file\n",
    "audio_file_path = r\"E:\\gamified_public_speaking\\test_gps_1.mp3\"\n",
    "\n",
    "try:\n",
    "    # Basic transcription\n",
    "    output = client.automatic_speech_recognition(\n",
    "        audio_file_path, \n",
    "        model=\"openai/whisper-large-v3\"\n",
    "    )\n",
    "    \n",
    "    print(\"Transcription:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Text: {output.text}\")\n",
    "    \n",
    "    print(\"\\nDetailed Chunks with Timestamps:\")\n",
    "    print(\"-\" * 40)\n",
    "    for chunk in output.chunks:\n",
    "        start_time = chunk.timestamp[0]\n",
    "        end_time = chunk.timestamp[1]\n",
    "        text = chunk.text\n",
    "        print(f\"[{start_time:4.1f}s - {end_time:4.1f}s]: {text}\")\n",
    "    \n",
    "    # Check for filler words and speech patterns\n",
    "    full_text = output.text.lower()\n",
    "    filler_words = ['um', 'uh', 'ah', 'like', 'you know', 'so', 'well', 'actually', 'basically', 'er', 'hmm', 'yeah']\n",
    "    \n",
    "    found_fillers = [word for word in filler_words if word in full_text]\n",
    "    \n",
    "    print(f\"\\nüìä Analysis:\")\n",
    "    print(f\"Languages detected: {output.inferred_languages}\")\n",
    "    \n",
    "    if found_fillers:\n",
    "        print(f\"‚úì Filler words/patterns detected: {found_fillers}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No obvious filler words detected\")\n",
    "    \n",
    "    # Look for speech patterns (hesitations, self-corrections)\n",
    "    speech_patterns = []\n",
    "    if \"...\" in output.text:\n",
    "        speech_patterns.append(\"hesitation/pause (...)\")\n",
    "    if \"meaning to\" in output.text:\n",
    "        speech_patterns.append(\"self-correction\")\n",
    "    if \"so,\" in output.text.lower():\n",
    "        speech_patterns.append(\"'so' as filler\")\n",
    "    if \"yeah\" in output.text.lower():\n",
    "        speech_patterns.append(\"'yeah' as filler\")\n",
    "    \n",
    "    if speech_patterns:\n",
    "        print(f\"‚úì Speech patterns detected: {speech_patterns}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c761568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dhari\\.cache\\huggingface\\hub\\models--openai--whisper-large-v3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "c:\\Users\\dhari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dhari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transcription (should preserve 'uh', 'um'):\n",
      " Hey guys, this is Dharanesh. I'm here to talk about myself. So I'm studying meaning to study data science. This is very nice.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import librosa\n",
    "\n",
    "# Load model locally (preserves more details)\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load and transcribe audio\n",
    "audio, sr = librosa.load(r\"E:\\gamified_public_speaking\\test_gps_1.mp3\", sr=16000)\n",
    "result = pipe(audio)\n",
    "\n",
    "print(\"Raw transcription (should preserve 'uh', 'um'):\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467eb67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transcription (should include 'uh', 'um'):\n",
      " Hey guys, this is Dharinesh. I'm here to talk about myself. So, I'm studying... meaning to study data science. Yeah, this is very nice.\n",
      "\n",
      "Cleaned transcription:\n",
      " Hey guys, this is Dharinesh. I'm here to talk about myself. So, I'm studying... meaning to study data science. Yeah, this is very nice.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Load model and processor\n",
    "model_name = \"openai/whisper-large-v3\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load audio\n",
    "audio, sr = librosa.load(r\"E:\\gamified_public_speaking\\test_gps_1.mp3\", sr=16000)\n",
    "\n",
    "# Process audio\n",
    "input_features = processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "# Generate with parameters that preserve filler words\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(\n",
    "        input_features,\n",
    "        max_length=448,\n",
    "        num_beams=1,  # Greedy decoding (less cleaning)\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        suppress_tokens=[],  # Don't suppress any tokens\n",
    "        condition_on_prev_tokens=False,  # More literal\n",
    "        compression_ratio_threshold=None,  # Disable compression filtering\n",
    "        logprob_threshold=None,  # Include uncertain words\n",
    "        no_speech_threshold=None  # Don't filter silence\n",
    "    )\n",
    "\n",
    "# Decode without skipping special tokens that might be filler words\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False, normalize=False)[0]\n",
    "\n",
    "print(\"Raw transcription (should include 'uh', 'um'):\")\n",
    "print(transcription)\n",
    "\n",
    "# Also try with normalized version\n",
    "transcription_clean = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nCleaned transcription:\")\n",
    "print(transcription_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32505a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading audio file...\n",
      "Audio uploaded successfully!\n",
      "Starting transcription...\n",
      "Processing...\n",
      "\n",
      "Transcription with filler words:\n",
      "--------------------------------------------------\n",
      "Learning is like riding a bicycle. Because once you get the hang of it, you never forget.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def transcribe_with_filler_words(audio_file_path):\n",
    "    API_TOKEN = os.getenv(\"assembly_key\")\n",
    "    headers = {'authorization': API_TOKEN}\n",
    "    \n",
    "    print(\"Uploading audio file...\")\n",
    "    \n",
    "    # Upload the audio file\n",
    "    with open(audio_file_path, 'rb') as f:\n",
    "        response = requests.post('https://api.assemblyai.com/v2/upload',\n",
    "                               headers=headers, files={'file': f})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return f\"Upload failed: {response.text}\"\n",
    "    \n",
    "    audio_url = response.json()['upload_url']\n",
    "    print(\"Audio uploaded successfully!\")\n",
    "    \n",
    "    # Request transcription with disfluencies enabled\n",
    "    data = {\n",
    "        'audio_url': audio_url,\n",
    "        'disfluencies': True,  # Preserves \"uh\", \"um\", etc.\n",
    "        'filter_profanity': False,\n",
    "        'punctuate': True\n",
    "    }\n",
    "    \n",
    "    print(\"Starting transcription...\")\n",
    "    response = requests.post('https://api.assemblyai.com/v2/transcript',\n",
    "                           json=data, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return f\"Transcription request failed: {response.text}\"\n",
    "    \n",
    "    transcript_id = response.json()['id']\n",
    "    \n",
    "    # Wait for completion\n",
    "    while True:\n",
    "        response = requests.get(f'https://api.assemblyai.com/v2/transcript/{transcript_id}',\n",
    "                              headers=headers)\n",
    "        result = response.json()\n",
    "        \n",
    "        if result['status'] == 'completed':\n",
    "            return result['text']\n",
    "        elif result['status'] == 'error':\n",
    "            return f\"Error: {result['error']}\"\n",
    "        \n",
    "        print(\"Processing...\")\n",
    "        time.sleep(3)\n",
    "\n",
    "# Your audio file path\n",
    "audio_file_path = r\"E:\\gamified_public_speaking\\user_1_1755468800.mp3\"\n",
    "\n",
    "# Transcribe\n",
    "try:\n",
    "    transcription = transcribe_with_filler_words(audio_file_path)\n",
    "    print(\"\\nTranscription with filler words:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(transcription)\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f7d7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API key: 3918b6714a...\n",
      "Status Code: 200\n",
      "‚úÖ API key is valid!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def test_assembly_ai_key():\n",
    "    \"\"\"Test if AssemblyAI API key is working\"\"\"\n",
    "    API_TOKEN = os.getenv(\"assembly_key\")\n",
    "    headers = {'authorization': API_TOKEN}\n",
    "    \n",
    "    print(f\"Testing API key: {API_TOKEN[:10]}...\" if API_TOKEN else \"No API key found!\")\n",
    "    \n",
    "    # Test with a simple API call\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            'https://api.assemblyai.com/v2/transcript',\n",
    "            headers=headers\n",
    "        )\n",
    "        \n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ API key is valid!\")\n",
    "            return True\n",
    "        elif response.status_code == 401:\n",
    "            print(\"‚ùå API key is invalid or expired\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"‚ùå Unexpected response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing API: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_assembly_ai_key()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
